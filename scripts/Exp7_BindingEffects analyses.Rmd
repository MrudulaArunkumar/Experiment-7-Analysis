---
title: "Binding level Analysis of Experiment 7"
author: "Carina & Mrudula"
date: "`r format(Sys.time(), '%d %B,%Y')`"
output:
  html_document:
    theme: readable
    highlight: breezedark
    toc: yes
    toc_float: yes
    fig_caption: yes
    fig_width: 7
    fig_height: 4
    code_folding: hide
---


### Data Preparation ###

The full dataset is loaded and the unnecessary columns are removed and the RT and ACC columns are prepared. 
The exclusion criteria (Tukey cut offs for outliers and RTs less than 200ms) are computed and the outlier RTs are removed.
This prepared data is then saved as **data_prepared.csv** and used for further analysis.
Participant 10 is removed from the dataset due to high number of errors (~25%)



```{r message=FALSE,warning=FALSE}
library(plyr)
library(lme4)
library(lmerTest)
library(tidyverse)
library(ez)
library(ggplot2)
library(schoRsch)
library(here)
library(sjPlot)
library(knitr)
library(pander)
library(rmarkdown)

```


Creating a new column (*binding sequence*) that identifies all the binding trials

- These are trials where Trial $n$ is a *test* trial (probe) and Trial $n-1$ is a *learn* trial (prime).
- Only accurate primes are used




```{r}
#read prepared file (outliers and errors excluded)
raw.data<-read.csv2(file=here("Data","data_prepared.csv"))

#exclude participant with too many errors
raw.data<-subset(raw.data, subset=(raw.data$participant!="10"))

#reduce dataset
raw.data<-raw.data%>%
  select(Validity, Saliency, Condition, SalD, NSalD, CorrectAnswer, Target, RT_Trials, ACC_trials, participant, RT_io)

#limit dataset to sequences with trial n-1=learn and trial n=test####
raw.data<-raw.data%>%
  mutate(binding_sequence= ifelse(Condition=="test" & lag(Condition,1)=="learn" & lag(ACC_trials,1)==1 & lag(participant,1)==participant, 1,0))

raw.data<-raw.data%>%
  select(binding_sequence, Condition, everything())

pander(table(raw.data$binding_sequence), style = "rmarkdown", caption = "Number of binding, prime-probe sequences")
#3832 relevant sequences

```

## Introducing the factors used for analysis ##

1. **DRel (Distractor Relation)**: 

-This factor is computed for these binding sequences where probe is a test trial and if the saliency is salient/nonsalient and the Salient/NonSalient Distractor of prime is same as the probe then the factor is *Distractor Repetition (DR)*
- For these binding sequences where probe is a test trial and if the saliency is salient/nonsalient and the Salient/NonSalient Distractor of prime is different from the probe, then the factor is *Distractor Change (DC)*


```{r factors, warning=FALSE, message=FALSE}
#code factors####
#DRel
raw.data<-raw.data%>%
  mutate(DRel =ifelse(binding_sequence==1 & Condition=="test" & Saliency=="Salient" & lag(SalD,1) == SalD,"DR",
                      ifelse(binding_sequence==1 & Condition=="test" & Saliency=="NonSalient" & lag(NSalD,1) == NSalD,"DR",
                             ifelse(binding_sequence==1 & Condition=="test" & Saliency=="Salient" & lag(SalD,1) != SalD,"DC",
                                    ifelse(binding_sequence==1 & Condition=="test" & Saliency=="NonSalient" & lag(NSalD,1) != NSalD,"DC",NA))))
         )

pander(table(raw.data$DRel), style = "rmarkdown", caption = "Distractor Relations for all binding sequences")
```


2. **RRel (Response Relation)**: 

-This factor is computed for these binding sequences where probe is a test trial and if prime's correct response is same as the probe then the factor is *Response Repetition (RR)*
- For these binding sequences where probe is a test trial and if the prime's correct response is different from the probe's , then the factor is *Response Change (RC)*


```{r rrel, message=FALSE, warning=FALSE}
#RREl
raw.data<-raw.data%>%
  mutate(RRel=ifelse(binding_sequence==1 & CorrectAnswer==lag(CorrectAnswer,1), "RR",
                     ifelse(binding_sequence==1 & CorrectAnswer!=lag(CorrectAnswer,1), "RC",NA)))

pander(table(raw.data$RRel), style = "rmarkdown", caption = "Response Relation for all the binding sequences")
```

3. **Probe Saliency**

- This is the same as the Saliency Factor but recoded in the binding scenario by coding the test trial's saliency as probe saliency

```{r probesal, message=FALSE,warning=FALSE}
#Probesalience
#equate to Saliency
raw.data<-raw.data%>%
  mutate(ProbeSaliency=ifelse(binding_sequence==1 & Condition=="test" &Saliency == "Salient", "salient",
                              ifelse(binding_sequence==1 & Condition=="test" &Saliency == "NonSalient", "nonsalient", NA)))

pander(table(raw.data$ProbeSaliency), style = "rmarkdown", caption = "Salient and Non Salient probes")
```


Now, the dataframe is reduced to only the binding sequences trials 

```{r}
#get rid of all trials except test trials in relevant binding sequence
raw.data<-subset(raw.data, subset=(raw.data$binding_sequence==1))
raw.data$err<-1-raw.data$ACC_trials

pander(table(raw.data$Validity), style = "rmarkdown", caption = "Number of valid and invalid trials in the SRB trials")
```



### Creating aggregate files ###

The following aggregate files are created:

1. Aggregate with RTs against the above three factors across participant
2. Aggregate with ERs against the above three factors across participant

*While adding the validity factor in the aggregate command there are too many missing values which does not help with the ezANOVA function, hence it was not included in these aggregate files*

3. Only valid test trials are selected from the main df, and an aggregate is created using that data frame (**only valid**) with the above three factors across participant. MLM will be used in this case to circumbent the multiple missing values

```{r}
agg.out <- aggregate(data = raw.data, RT_io ~ DRel + RRel + ProbeSaliency  + participant,mean)
agg.err <- aggregate(data = raw.data, err ~ DRel + RRel + ProbeSaliency  + participant, mean)


#
#limit analyses only to valid test trials to avoid problem of too many missings
raw.dataV<-subset(raw.data, subset=(raw.data$Validity=="valid"))
OnlyVal_agg.out<- aggregate(data = raw.dataV, RT_io ~ DRel + RRel + ProbeSaliency  + participant, mean)
#too many empty cells to carry out ANOVA -> Model at the trial level to circumvent problem of missing data

#write.csv2(OnlyVal_agg.out, file="Data/val_agg_out.csv", row.names = F)

```

# ANOVAs

## 1. ANOVA: DRel, RRel, Probe saliency for binding trials with RTs ##

> The three way interaction is significant, $F = 8.10, p = 0.005$

```{r message=FALSE,warning=FALSE}
anova1 <- ezANOVA(data = agg.out,
                  dv = RT_io,
                  wid = .(participant),
                  within = .(DRel,RRel,ProbeSaliency),
                  detailed = TRUE)
panderOptions('table.split.table',300)
pander(anova1, style = "rmarkdown", caption = "ANOVA (RT): for binding trials with DRel, RRel, ProbeSaliency")

plotDRRR <- ggplot(data = agg.out,aes(x = RRel, y = RT_io, color = DRel))+
  geom_line(aes(group = DRel, linetype = DRel),size = 1,stat = "summary", fun = "mean",)+
  geom_point(stat = "summary", fun = "mean", aes(shape = DRel))+
  facet_grid(.~ProbeSaliency)+labs(color  = "Distractor Relation", linetype = "Distractor Relation", shape = "Distractor Relation")+
  scale_color_manual(values = c("deepskyblue4","cadetblue3"))+
  coord_cartesian(ylim = c(500,600))+
  theme_classic()+ylab("ProbeRT (in ms)")+xlab("Response Relation")+ggtitle("Interaction of Response Relation and \n Distractor Relation across Saliency")
plotDRRR

```

The DRel X RRel effects are significant for the salient trials. Below are the ANOVA results only with salient trials and the mean differences in the binding/retrieval effects.
There is a facilitation when the Distractor repeats (when the response repeats) of 39ms which is significant but the cost is not significant.

```{r warning=FALSE, message=FALSE}
SalientProbe<-subset(agg.out, subset=(ProbeSaliency=="salient"))
sal<-ezANOVA( data=SalientProbe,
                dv=RT_io,
                wid = participant,
                within= .(DRel, RRel),
                detailed=T)
panderOptions('table.split.table',300)
pander(sal, style = "rmarkdown", caption = "Only for salient trials")

meansal <- ezStats(data=SalientProbe,
        dv=RT_io,
        wid = participant,
        within = .(RRel),
        within_full = .(DRel, RRel),
        diff=.(DRel),
        reverse_diff = T)

pander(meansal, style = "rmarkdown", caption = "Mean differences between DC - DR for Response Relation only for salient trials")

#t.tests:
pander(t.test(RT_io ~ DRel, data =SalientProbe, subset =(RRel=="RR"), paired=TRUE, alternative="greater"),style = "rmarkdown", caption = "T-test results between DC and DR, only for Response Repetition trials" )

#facilitation sign

pander(t.test(RT_io ~ DRel, data =SalientProbe, subset =(RRel=="RC"), paired=TRUE, alternative="less"), style = "rmarkdown", caption = "T-test resuults between DC and DR, for Response Change trials (Costs)")

#cost ns
```

..and the effects for only the non salient trials are not significant.

```{r warning=FALSE, message=FALSE}

NonSalientProbe<-subset(agg.out, subset=(ProbeSaliency=="nonsalient"))
nonsal<-ezANOVA( data=NonSalientProbe,
                dv=RT_io,
                wid = participant,
                within= .(DRel, RRel),
                detailed=T)
panderOptions('table.split.table',300)
pander(nonsal, style = "rmarkdown", caption = "ANOVA for only non salient trials")

```


## 2. ANOVA: DRel, RRel, Probe saliency for binding trials with ERs

> The three way interaction is significant, $F = 4.91,p = 0.03$. The two way interaction between DRel X RRel is also significant, $F = 4.46, p = 0.03$


```{r warning=FALSE, message=FALSE}
anova2 <- ezANOVA(data = agg.err,
                  dv = err,
                  wid = .(participant),
                  within = .(DRel,RRel,ProbeSaliency),
                  detailed = TRUE)
panderOptions('table.split.table',300)
pander(anova2, style = "rmarkdown", caption = "ANOVA (ERs): for binding trials DRel, RRel, Probe Saliency")
```

Similar to RTs, while looking at the salient trials alone, the DRel x RRel effect is significant $F = 7.5, p = 0.007$ 
Below are the ANOVA results along with the t-test results for each of the Response Relation to tease apart the facilitations and costs.
The facilitation is marginally significant but the cost is not significant. 

```{r warning=FALSE, message=FALSE}
SalientProbeER<-subset(agg.err, subset=(ProbeSaliency=="salient"))
salER<-ezANOVA( data=SalientProbeER,
                dv=err,
                wid = participant,
                within= .(DRel, RRel),
                detailed=T)
panderOptions('table.split.table',300)
pander(salER, style = "rmarkdown", caption = "Only for salient trials")

meansalER <- ezStats(data=SalientProbeER,
        dv=err,
        wid = participant,
        within = .(RRel),
        within_full = .(DRel, RRel),
        diff=.(DRel),
        reverse_diff = T)
panderOptions('table.split.table',300)
pander(meansalER, style = "rmarkdown", caption = "Mean differences between DC - DR for Response Relation only for salient trials")

#t.tests:
pander(t.test(err ~ DRel, data =SalientProbeER, subset =(RRel=="RR"), paired=TRUE, alternative="greater"),style = "rmarkdown", caption = "T-test(ER) results between DC and DR, only for Response Repetition trials" )

#facilitation sign

pander(t.test(RT_io ~ DRel, data =SalientProbe, subset =(RRel=="RC"), paired=TRUE, alternative="less"), style = "rmarkdown", caption = "T-test resuults between DC and DR, for Response Change trials (Costs)")

#cost ns
```

## MULTI LEVEL MODELLING

....to be discussed and finalized
 
 
```{r}
#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# #analysis at the trial level####
# 
# pander(table(raw.dataV$DRel),style = "rmarkdown", caption = "Distractor Relations with only Valid probes")
# pander(table(raw.dataV$RRel),style = "rmarkdown", caption = "Response Relations with only Valid probes")
# pander(table(raw.dataV$ProbeSaliency),style = "rmarkdown", caption = "Probe saliency with only Valid probes")
# pander(table(raw.dataV$Validity),style = "rmarkdown", caption = "Valid and invalid Probes")

#create numeric variables for Multilevel analysis
# 
# raw.dataV$drel<-ifelse(raw.dataV$DRel=="DR", 1, -1)
# # table(raw.dataV$drel, raw.dataV$DRel)
# # summary(raw.dataV$drel)
# 
# raw.dataV$probesal<-ifelse(raw.dataV$ProbeSaliency=="salient", 1, -1)
# #table(raw.dataV$probesal, raw.dataV$ProbeSaliency)
# #summary(raw.dataV$probesal)
# 
# raw.dataV$val<-ifelse(raw.dataV$Validity=="valid", 1, -1)
# #table(raw.dataV$val, raw.dataV$Validity)
# #summary(raw.dataV$val) #but already all invalid trials are removed
# 
# raw.dataV$rrel<-ifelse(raw.dataV$RRel=="RR", 1, 2)
# table(raw.dataV$rrel, raw.dataV$RRel)
# 
# 
# #centering within person for RREl
# #previous_rm
# means.rrel <- aggregate(data = raw.dataV, rrel ~ participant, mean)
# names(means.rrel)[2]<-"rrel_mean"
# 
# raw.dataV<-merge (raw.dataV, means.rrel, by="participant")
# 
# raw.dataV$rrel_cwp <- raw.dataV$rrel-raw.dataV$rrel_mean
# #summary(raw.dataV$rrel_cwp)

#random slopes model, Probe RT
# #RT
# randomSlopes_m1<-lmer(RT_io~1+probesal*drel*rrel_cwp + (1+probesal*drel*rrel_cwp|participant), 
#                       data=raw.dataV, 
#                       REML=F,
#                       na.action = "na.omit")
# tab_model(randomSlopes_m1, show.se = TRUE,show.stat = TRUE, show.ci = FALSE)
# summary(randomSlopes_m1)
# #threeway interaction sign

# #errors
# 
# randomSlopes_m1_err<-lmer(err~1+probesal*drel*rrel_cwp + (1+probesal*drel*rrel_cwp|participant), 
#                       data=raw.dataV, 
#                       REML=F,
#                       na.action = "na.omit")
# 
# summary(randomSlopes_m1_err)
# #threeway interaction sign
# #so far: MLM analyses correspond to analyses at the aggregated level!
# 
# #+++++++++++++++++++++++
# #incorporate test trial validity to the model
# 
# #RT
# randomSlopes_m2<-lmer(RT_io~1+probesal*drel*rrel_cwp*val + (1+probesal*drel*rrel_cwp*val|participant), 
#                       data=raw.dataV, 
#                       REML=F,
#                       na.action = "na.omit")
# tab_model(randomSlopes_m2, show.se = TRUE,show.stat = TRUE, show.ci = FALSE)
# summary(randomSlopes_m2)
# #threeway interaction sign
# 
# #errors
# 
# randomSlopes_m2_err<-lmer(err~1+probesal*drel*rrel_cwp*val + (1+probesal*drel*rrel_cwp*val|participant), 
#                           data=raw.dataV, 
#                           REML=F,
#                           na.action = "na.omit")
# 
# summary(randomSlopes_m2_err)
# 
# 
# #test model fit
# randomIntercept_m0<-lmer(RT_io~1 + (1|participant), 
#                          data=raw.dataV, 
#                          REML=F,
#                          na.action = "na.omit")
# summary(randomIntercept_m0)
# 
# randomIntercept_m0_err<-lmer(err~1 + (1|participant), 
#                              data=raw.dataV, 
#                              REML=F,
#                              na.action = "na.omit")
# summary(randomIntercept_m0_err)
# 
# #Model comparison
# #Model 1 vs Model0
# 
# anova(randomIntercept_m0, randomSlopes_m1) #sign
# anova(randomIntercept_m0_err, randomSlopes_m1_err) #sign
# 
# #Model 2 vs model 1
# anova(randomSlopes_m1, randomSlopes_m2) #no sign model improvement
# anova(randomSlopes_m1_err, randomSlopes_m2_err) #no sign model improvement
# 
# #check for confunds in the factorial design####
# table(raw.dataV$DRel, raw.dataV$RRel )
# round(table(raw.dataV$DRel, raw.dataV$RRel )/nrow(raw.dataV)*100, digits = 2)
# 
# table(raw.dataV$DRel, raw.dataV$RRel, raw.dataV$ProbeSaliency )
# round(table(raw.dataV$DRel, raw.dataV$RRel, raw.dataV$ProbeSaliency )/nrow(raw.dataV)*100, digits = 1)
# 
# round(table(raw.dataV$DRel, raw.dataV$RRel, raw.dataV$ProbeSaliency, raw.dataV$Validity )/nrow(raw.dataV)*100, digits = 1)
# 
# table(raw.dataV$DRel, raw.dataV$RRel, raw.dataV$Validity, raw.dataV$ProbeSaliency) 
# 
# table(raw.dataV$DRel, raw.dataV$RRel, raw.dataV$Validity) 
# round(table(raw.dataV$DRel, raw.dataV$RRel, raw.dataV$Validity) /nrow(raw.dataV)*100, digits = 1)
# 


```