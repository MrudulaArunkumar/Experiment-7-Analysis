---
title: "Multi level model Analysis of Experiment 7"
author: "Mrudula & Carina"
date: "`r format(Sys.time(), '%d %B,%Y')`"
output:
  html_document:
    theme: readable
    highlight: breezedark
    toc: yes
    toc_float: yes
    fig_caption: yes
    fig_width: 7
    fig_height: 4
    code_folding: hide
---


```{r load libs, message=FALSE, warning=FALSE}
library(plyr)
library(lme4)
library(lmerTest)
library(tidyverse)
library(knitr)
library(pander)
library(rmarkdown)
library(here)
library(sjPlot)

Exp7data <- read.csv(here("Data","Exp7_fulldataset.csv"))
Exp7data$participant <- as.factor(Exp7data$participant)
```


### Data preparation and cleaning ###

-   Removing unnecessary columns generated by psychopy
-   Preparing the RT trial, by eliminating the square brackets and splitting it in cases where two keys were registered.
-   Creating a column for Accuracy and Error Rate
-   adding a Bonus column that computes the points received by each participant

```{r clean, message=FALSE, warning=FALSE}
Exp7data <- Exp7data %>%
  select(-X,-ConsentKey.keys,-ConsentKey.rt,-Begin.keys,-Begin.rt,-checkresp.corr,-checkresp.keys,-checkresp.rt,-Attention.thisRepN,-Attention.thisTrialN,-Attention.thisN,-Attention.thisIndex,-Attention.ran,-AttnQuestion,-AttnAnswer,-NextStep.keys,-NextStep.rt,-InstRep.ran,-InstRep.thisN,-InstRep.thisTrialN,-InstRep.thisRepN,-InstRep.thisIndex,-PracProceed.keys,-PracProceed.rt,-Prac_loop.thisRepN,-Prac_loop.thisTrialN,-Prac_loop.thisN,-Prac_loop.thisIndex,-Prac_loop.ran,-Exp_proceed.keys,-Exp_proceed.rt,-PracRepeat.ran,-PracRepeat.thisRepN,-PracRepeat.thisN,-PracRepeat.thisIndex,-PracRepeat.thisTrialN,-brkContinue.keys,-PauseResp.keys,-PauseResp.rt,-CALearntrials.thisRepN,-CALearntrials.ran,-CALearntrials.thisTrialN,-CALearntrials.thisIndex, -CA_Proceed.keys,-CA_Proceed.rt,-headstartLearn.thisRepN,-headstartLearn.thisTrialN,-headstartLearn.thisIndex,-headstartLearn.thisN,-headstartLearn.ran,-ExpTrials.ran,-ExpTrials.thisIndex,-CA_trials.thisRepN,-CA_trials.thisN,-CA_trials.thisIndex,-CA_trials.thisTrialN,-CA_trials.ran,-AwareQ_loop.thisRepN,-AwareQ_loop.ran,-AwareQ_loop.thisIndex,-AwareQ_loop.thisTrialN,-todebrief.keys,-Finalend.keys)



#adjusting RT
Exp7data$mainRT <- Exp7data$TargetResp.rt
Exp7data$Block1RT <- Exp7data$CAResponse.rt

#splitting the RTs from main block
Exp7data <- separate(Exp7data, col = mainRT, into = c("RTm_Trials", "RTm_secondary"), sep = ',')
Exp7data$RTm_Trials <- Exp7data$RTm_Trials%>%
  str_replace_all("\\[|\\]","")%>%
  as.double(Exp7data$RTm_Trials)
Exp7data$RTm_Trials <- 1000*(Exp7data$RTm_Trials)

#splitting RTs from the Ca learn block (1st block)
Exp7data <- separate(Exp7data, col = Block1RT, into = c("RTb_Trials", "RTb_secondary"), sep = ',')
Exp7data$RTb_Trials <- Exp7data$RTb_Trials%>%
  str_replace_all("\\[|\\]","")%>%
  as.double(Exp7data$RTb_Trials)
Exp7data$RTb_Trials <- 1000*(Exp7data$RTb_Trials)

#removing RTs from guessing trials 
Exp7data$RTb_Trials <- ifelse(Exp7data$Block == "CG Learn", NA, Exp7data$RTb_Trials)

#creating a dummy variable to pick wherever the RTs are from main block and which are from first block
Exp7data$RTdummy <- ifelse(is.na(Exp7data$TargetResp.rt) == FALSE,1,NA)
Exp7data$RTdummy <- ifelse(is.na(Exp7data$CAResponse.rt)==FALSE,2,Exp7data$RTdummy)

#combining all important RTs 
Exp7data <- Exp7data %>%
  mutate(RT_Trials = ifelse((RTdummy == 1), RTm_Trials,ifelse((RTdummy == 2),RTb_Trials,NA)))


###creating a separate df with the contingency awareness
Exp7_CA <- Exp7data%>%
  filter(Target == "?" | str_detect(AwareQ, "Press"))





Exp7data <- Exp7data%>%drop_na(RT_Trials)

#combining accuracy from first block and main block
Exp7data$mainAcc <- Exp7data$TargetResp.corr
Exp7data$Block1Acc <- Exp7data$CAResponse.corr
Exp7data$BlockAcc <- ifelse(Exp7data$Block == "CG Learn", NA, Exp7data$Block1Acc)

Exp7data$ACCdummy <- ifelse(is.na(Exp7data$TargetResp.corr)==FALSE,1,NA)
Exp7data$ACCdummy <- ifelse(is.na(Exp7data$CAResponse.corr)==FALSE,2,Exp7data$ACCdummy)

Exp7data <- Exp7data %>%
  mutate(ACC_trials = ifelse((ACCdummy == 1),mainAcc,ifelse((ACCdummy == 2),Block1Acc,NA)))


Exp7data$ErrorRate <- 1 - Exp7data$ACC_trials

#Error rate
pander(table(Exp7data$ACC_trials), style = "rmarkdown", caption = "Total number of Accuracy trials")
pander(round(table(Exp7data$ACC_trials)/nrow(Exp7data)*100, digits = 3), style = "rmarkdown",caption = "Percentage of accuracy and errors")
```


Upon implementing the exclusion criteria

- Outliers (1.5x above third quartile) --> used for the analysis
- Farouts (3x)above third quartile)
- very Fast RTs, less than 200ms

```{r exclusions, message=FALSE,warning=FALSE}

#Exclude errors from RT
Exp7data <- Exp7data %>%
  filter(participant != 10)
Exp7data$RT_Trials[Exp7data$ACC_trials==0] <- NA
pander(summary(Exp7data$RT_Trials), style = "rmarkdown", caption = "Mean of RTs before exclusions")


#exclude outliers
#creating function to remove the outliers and farouts
computeTukeys <- function(x){
  P25 <- quantile(x$RT_Trials, .25, na.rm = TRUE, type = 6) #type = 6 -> used in SPSS
  P75 <- quantile(x$RT_Trials, .75, na.rm = TRUE, type = 6)
  x$Outlier <- P75 + 1.5*(P75 - P25)
  x$Farouts <- P75 + 3.0*(P75 - P25)
  return(x)
}


#identifying the outliers and farouts at individual level
Exp7data <- ddply(Exp7data, .(participant), computeTukeys)

#creating new column with RT trials after removing outliers/farouts
Exp7data$RT_io <- Exp7data$RT_Trials

Exp7data$RT_io[Exp7data$RT_io > Exp7data$Outlier|Exp7data$RT_io < 200] <- NA
pander(summary(Exp7data$RT_io), style = "rmarkdown", caption = "Mean RTs after removing outliers")


```


## Previous Occurence Analysis : Distance & Response Type

### Distance

This variable tells how far ago the last occurence of distractor appear

The conditions used for these in comparsion to current trial $n$ within participant were as follows:

1. whether Trial $n-1$ or $n-x$ was a *learn* or a *test* trial.
2. if either the Salient Distractor or Non salient Distractor appeared in Trial $n-1$ or $n-x$
3. if Trial $n-1$ / $n-x$ was an accurate trial

and $x$ was saved as the Distance variable if it satisified **all** the above conditions where $x$ refers to the xth number of the trial behind $n$


```{r message=FALSE, warning=FALSE}

#add previous_respoonse and distance to the model####
Exp7data <- Exp7data%>%select(Condition, SalD,NSalD,Saliency,Validity, everything())

###first is to find out the trials where the previous occurence wa the immediate previous one
Exp7data$Distance <- NA
Exp7data <- Exp7data%>%select(Distance,ACC_trials,everything())
Exp7data <- Exp7data%>%
  mutate(Distance = ifelse((lag(Condition,1)=="test" | lag(Condition,1) == "learn")& 
                             (lag(SalD,1)== SalD|lag(NSalD,1)==NSalD) &
                             lag(participant,1)==participant &
                             lag(ACC_trials,1)== 1, 1, Distance))


#The number of immediate previous occurences

pander(table(Exp7data$Distance), style = "rmarkdown", caption = "Number of immediately preceding previous occurences")

## Now to look at other distances of the last occurrence beyond the immediately preceding one
lagvalue <- 2:20

for(j in lagvalue){
  Exp7data <- Exp7data %>% 
    mutate(Distance = ifelse((lag(Condition,j)=="learn"|lag(Condition,j)=="test") &  
                             (lag(SalD,j)==SalD|lag(NSalD,j)==NSalD) & lag(participant,j)==participant &
                              lag(ACC_trials,j)== 1 & is.na(Distance)==TRUE, j, Distance))
}

pander(table(Exp7data$Distance), style = "rmarkdown", caption = "Number of previous occurences at each distance")

```


### Previous Response ###

Coding what the previous Response was, whether it was the same or different. This variable is defined as Response Type, which has two factors: Response Change(RC) and Response Repetition (RR)

This variable is computed based on the following conditions (only with respect to Trial $n$ and Trial $n-1$):

1. whether the previous trial was *learn* or a *test* trial
2. whether the same Salient or Nonsalient Distractor appeared in both Trial $n$ and Trial $n-1$ 
3. Whether the Correct Answer for Trial$n-1$ is same as Trial $n$ (then RR) or different from Trial $n$ (then RC)

 *To be noted: This does not equate to validity* because the SRB trials can have the combination of invalid prime and invalid probe --> Response Repetition which is confounded by Validity. 
So every valid probe can have both the Response Type factors based on whether the prime is valid or invalid

 
```{r resptype, message=FALSE, warning=FALSE}

Exp7data$ResponseType <- NA

Rmlag <- 1:30
for(k in Rmlag){
  Exp7data <- Exp7data %>% 
    mutate(ResponseType = ifelse((lag(Condition,k)=="learn" | lag(Condition,k)=="test") &  
                                   (lag(SalD,k)==SalD|lag(NSalD,k)==NSalD) & lag(participant,k)==participant & 
                                   lag(CorrectAnswer,k)== CorrectAnswer & is.na(ResponseType)==TRUE, "RR", 
                                 ifelse((lag(Condition,k)=="learn"|lag(Condition,k)=="test") & (lag(SalD,k)==SalD|lag(NSalD,k)==NSalD)& lag(participant,k)==participant & lag(CorrectAnswer,k)!= CorrectAnswer & is.na(ResponseType)==TRUE, "RC", ResponseType)))
}

pander(table(Exp7data$ResponseType),style = "rmarkdown", caption = "Number of Response Repetitions and Change")

Exp7data <- Exp7data%>%select(ResponseType,RT_io,CorrectAnswer, everything())


```


# MULTI LEVEL MODELLING #

1. Creating a dataframe copying the entire dataset with all 70 participants and all the conditions.
2. Changing the predictor levels to numeric type

  - Validity --> Val = 1,2 (valid, invalid)
  - ResponseType --> previous_rm = 1,2 (RR,RC)
  - Distance is already numeric

3. creating a standardised score for the predictors: ZVal, Zprevious_rm, zDistance
  
```{r mlm, message=FALSE}


#Compute first model: level1 predictor: probe compatiblity; level2 predictor: subject;
#random slopes, predict RT with contingency factor (high vs low) = aka "probe_comp" var ####


raw.data<-Exp7data

#compute new numeric var for validity effect
raw.data$val <-ifelse(raw.data$Validity=="valid",1,2)
pander(table(raw.data$Validity,raw.data$val), style = "rmarkdown", caption = "Numeric values of validity")

#create numerical var for response type

raw.data$previous_rm<-ifelse(raw.data$ResponseType=="RR", 1, 2)
pander(table(raw.data$ResponseType, raw.data$previous_rm), style = "rmarkdown", caption = "Numeric values of Response Type")


#center predictors

raw.data$Zval=scale(raw.data$val, scale=FALSE)
raw.data$Zprevious_rm=scale(raw.data$previous_rm, scale=F)
raw.data$ZDistance=scale(raw.data$Distance, scale=F)

summary(raw.data$Distance)

```

## 0. Null model

```{r}

randomIntercept_m0<-lmer(RT_io~1 + (1|participant), 
                         data=raw.data, 
                         REML=F,
                         na.action = "na.omit")
#tab_model(randomIntercept_m0, show.ci = FALSE,show.stat = TRUE)
```



## 1. Model 1: Only with Validity as Level 1 Predictor with intercept varying over participant

 > Validity has a significant effect
 
 This contains the valid and invalid trials across the whole experiment including learn and test

```{r m1, message=FALSE, warning=FALSE}


randomSlopes_m1<-lmer(RT_io~1+val + (1+val|participant), 
                      data=raw.data, 
                      REML=F,
                      na.action = "na.omit")

summary(randomSlopes_m1)

sjPlot::tab_model(randomSlopes_m1, show.se = TRUE,show.stat = TRUE, show.ci = FALSE, 
                  pred.labels = c("Intercept","Validity"),
                  dv.labels = "Reaction Time")


```


## 2. Model 2: Adding the Factors of Previous Response (RR or RC) to the previous model

Using the standardised scores of the predictors (*centering is not effective because RR and RC are not equal in number*)

This also includes trials that are both learn and test, both invalid and valid.


```{r}

randomSlopes_m2<-lmer(RT_io~1+Zval + Zprevious_rm + (1+Zval +Zprevious_rm |participant), 
                      data=raw.data, 
                      REML=F,
                      na.action = "na.omit")

sjPlot::tab_model(randomSlopes_m2, show.se = TRUE,show.stat = TRUE, show.ci = FALSE, 
                  pred.labels = c("Intercept","Validity","ResponseMode"),
                  dv.labels = "Reaction Time")
summary(randomSlopes_m2)


```


## 3. Model 3: Introducting Distance as a predictor

**To be noted** :
Distance is shorter (1 or 2) in the beginning of the experiment due to the headstart learn trials that are also only valid. 
So two models were run
1. With Distance as a predictor using all the trials
2. With the headstart and Saliency  manipulation trials removed so that it includes the experiment trials that have the intermixture of learn and test (*although the first 80 trials are valid and invalid learn trials*)

### 3a. Validity, ResponseType and Distance as predictors for all the RTs and after removing headstart block

```{r}

#run model without head start trials (40 valid only trials)
raw.data<-subset(raw.data, subset = (raw.data$Block!="HeadStart" & raw.data$Block!="SalMC"))

randomSlopes_m3<-lmer(RT_io~1+Zval + Zprevious_rm + ZDistance + (1+Zval +Zprevious_rm +ZDistance |participant), 
                      data=raw.data, 
                      REML=F,
                      na.action = "na.omit")


#run model without head start trials (40 valid only trials)
raw.data<-raw.data %>%
  subset(Block = c("HeadStart","SalMC"))

randomSlopes_m4<-lmer(RT_io~1+Zval + Zprevious_rm + ZDistance + (1+Zval +Zprevious_rm +ZDistance |participant), 
                      data=raw.data, 
                      REML=F,
                      na.action = "na.omit")


tab_model(randomSlopes_m3,randomSlopes_m4, show.se = TRUE,show.stat = TRUE, show.ci = FALSE, 
                  dv.labels = c("Distance, \n Validity, Response for all RTs","Distance, \n Validity, Response for RTs (minus headtstart block)"))

```

### 3b. Introducing the interaction Factor of ResponseType & Distance

> The interaction between ResponseType and Distance is significant.

*But the t-statistic is negative*, probably because most of the shorter distances are below the mean Distance `r table(raw.data$Distance)` that makes it below 0 while standardising. 

```{r}

#incorporate interaction between previous_rm and distance
randomSlopes_m5<-lmer(RT_io~1+Zval + Zprevious_rm*ZDistance + (1+Zval +Zprevious_rm*ZDistance |participant), 
                      data=raw.data, 
                      REML=F,
                      na.action = "na.omit")

#summary(randomSlopes_m5)

tab_model(randomSlopes_m5, show.se = TRUE,show.stat = TRUE, show.ci = FALSE, 
                  dv.labels = c("Interaction of Response and Distance"))

#Doublecheck meaning of distance*previous_rm interaction with Klaus: Due to bigger difference of RR vs RC fOr distance of 1 and distance of 2 (which fall below 0 when transformed to Z scores?) 
#compared to more distant distances (there, the difference between RR and RC should be much smaller...)

```

### MODEL COMPARISON

```{r}

pander(anova(randomSlopes_m4,randomSlopes_m5))

```

### 4. Model 4: For the RTs of Salient Distractors (which show a validity effect)

So for this purpose the dataset was adjusted by removing all the learn trials and only assessing the test trials that had Salient distractors

*However, the distance and Response Type variable computed earlier still took into consideration the intermixture of learn and test trials that the value of the variable still includes the factor of previous trials' condition being learn or not*

### 4a. Only with Validity as a predictor

Validity has a significant effect

```{r}

####Run MLM only for salient D's (because only these show the validity effect) ###

table(raw.data$Condition, raw.data$Saliency)
raw.dataSD<-subset(raw.data, subset=(raw.data$Condition=="test" & raw.data$Saliency=="Salient"))

randomSlopes_m6<-lmer(RT_io~1+val + (1+val|participant), 
                      data=raw.dataSD, 
                      REML=F,
                      na.action = "na.omit")

tab_model(randomSlopes_m6, show.stat = TRUE, show.ci = FALSE,
          pred.labels = c("Intercept","Validity"),
          dv.labels = "Reaction Time")

```

### 4b. With ResponseType and Distance as predictors along with validity

Distance does not signficantly moderate the RTs but Validity and Response Type do.


```{r}
randomSlopes_m7<-lmer(RT_io~1+Zval + Zprevious_rm + ZDistance + (1+Zval +Zprevious_rm +ZDistance |participant), 
                      data=raw.data, 
                      REML=F,
                      na.action = "na.omit")

tab_model(randomSlopes_m7, show.stat = TRUE,show.ci = FALSE,dv.labels = "Validity,Response and Distance on RTs")

```

### 4c. with interaction of Distance and Response

```{r}

#incorporate interaction between previous_rm and distance
randomSlopes_m8<-lmer(RT_io~1+Zval + Zprevious_rm*ZDistance + (1+Zval +Zprevious_rm*ZDistance |participant), 
                      data=raw.data, 
                      REML=F,
                      na.action = "na.omit")

tab_model(randomSlopes_m8, show.ci = FALSE,show.stat = TRUE, dv.labels = "Interaction included")


```
Summary of  **model 4**

```{r}
tab_model(randomSlopes_m7,randomSlopes_m8, show.stat = TRUE,show.ci = FALSE)
```

